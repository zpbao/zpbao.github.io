<!DOCTYPE html>
<html lang="en">
<head>
    <title>Zhipeng Bao</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Zhipeng Bao">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="Zhipeng Bao's Homepage" />
    <meta name="keywords" content="Zhipeng Bao" />

    <link rel="shortcut icon" href="../favicon.ico">

    <!--CSS styles-->
    <link rel="stylesheet" href="css/bootstrap.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <link rel="stylesheet" href="css/style.css">
    <link id="theme-style" rel="stylesheet" href="css/styles/default.css">


    <!--/CSS styles-->
    <!--Javascript files-->
    <script type="text/javascript" src="js/jquery-1.10.2.js"></script>
    <script type="text/javascript" src="js/TweenMax.min.js"></script>
    <script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
    <script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>

    <script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
    <script type="text/javascript" src="js/jquery.dropdownit.js"></script>

    <script type="text/javascript" src="js/jquery.stellar.min.js"></script>
    <script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

    <script type="text/javascript" src="js/bootstrap.min.js"></script>

    <script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

    <script type="text/javascript" src="js/masonry.min.js"></script>

    <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>

    <script type="text/javascript" src="js/magnific-popup.js"></script>
    <script type="text/javascript" src="js/custom.js"></script>

    <!--/Javascript files-->

</head>
<body>

<div id="wrapper">
    <a href="#sidebar" class="mobilemenu"><i class="icon-reorder"></i></a>

    <div id="sidebar">
        <div id="main-nav">
            <div id="nav-container">
                <div id="profile" class="clearfix">
                    <div class="portrate hidden-xs"></div>
                    <div class="title">
                        <h2>Zhipeng Bao</h2>
                        <h3>Carnegie Mellon University</h3>
                    </div>

                </div>
                <ul id="navigation">
                    <li>
                        <a href="index.html">
                            <div class="icon icon-user"></div>
                            <div class="text">About Me</div>
                        </a>
                    </li>

                    <li class="currentmenu">
                        <a href="research.html">
                            <div class="icon icon-book"></div>
                            <div class="text">Research</div>
                        </a>
                    </li>

                    <li>
                        <a href="publication.html">
                            <div class="icon icon-edit"></div>
                            <div class="text">Publications</div>
                        </a>
                    </li>

                    <li>
                        <a href="gallery.html">
                            <div class="icon icon-picture"></div>
                            <div class="text">Gallery</div>
                        </a>
                    </li>

                    <li>
                        <a href="contact.html">
                            <div class="icon icon-calendar"></div>
                            <div class="text">Contact Me</div>
                        </a>
                    </li>

                    <li>
                        <a href="cv-zhipengbao.pdf">
                            <div class="icon icon-download-alt"></div>
                            <div class="text">Download CV</div>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="social-icons">
            <ul>
                <li><a href="https://www.facebook.com/profile.php?id=100010655260380/"><i class="icon-facebook"></i></a></li>
                <li><a href="https://www.linkedin.com/in/zhipeng-bao-11030016a/"><i class="icon-linkedin"></i></a></li>
                <li><a href="https://github.com/zpbao"><i class="icon-github"></i></a></li>
            </ul>
        </div>
    </div>

<div id="main">

    <div id="research" class="page">
        <div class="pageheader">

            <div class="headercontent">

                <div class="section-container">
                    <h3 class="title">Research Projects</h2>
                </div>
            </div>
        </div>

        
        <div class="pagecontents">
            <div class="section color-2">
                <div class="section-container">

                    <div class="row">
                        <div class="col-md-12 col-md-offset-0">
                            <ul class="ul-withdetails">

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/key.png" class="img-responsive">
                                                <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div>

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <div class="meta">
                                                <h3>Generative Modeling for Multi-task Visual Learning</h3>
                                                <p>June 2020 - Dec. 2020</p>
                                                <p class="paraIndent">Supervisor: Martial Hebert, Professor, CMU.</p>
                                                <p>Apply genertaive networks to facilitate multi-task visual learning.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="details">
                                        <img src="files/multitask.png" width="100%"> </p>
                                        <p>Generative modeling has recently shown great promise in computer vision, but it has mostly focused on synthesizing visually realistic images. In this project, motivated by multi-task learning of shareable feature representations, we consider a broader problem of learning a shared genera- tive model that is useful across various visual perception tasks. </p>
                                        <p>we propose a general multi-task oriented generative modeling (MGM) framework, by coupling a discriminative multi-task network with a generative network. While it is challenging to synthesize both RGB images and pixel-level annotations in multi-task scenarios, our framework enables us to use synthesized images paired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks. Experimental evaluation on challenging multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework improves the performance of all the tasks by large margins, consistently outperforming state-of-the-art multi-task approaches.</p>
                                        <p>We have sumitted a paper to CVPR 2021 regarding this project. See the paper <a target="_blank" href="files/multitask.pdf"><strong>here</strong></a></p>
                                    </div>
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/chart.png"  class="img-responsive">
                                                <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div>

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <div class="meta">
                                                <h3>Generative Modeling for Joint Learning of Recognition and View-syntheis</h3>
                                                <p>Dec 2019 - June. 2020</p>
                                                <p class="paraIndent">Supervisor: Martial Hebert, Professor, CMU.</p>
                                                <p>Provide an effective solution for the novel task of jointly recognition and view-synthesis, the model works well in low-data regime.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="details">
                                        <img src="files/bowtie.png" width="100%"> </p>
                                        <p class="paraIndent normalcolor"> Generative modeling has recently shown great promise in computer vision, but its success is often limited to separate tasks. Motivated by multi-task learning of shareable feature representations, we consider a novel problem of learning a shared generative model across various tasks. We instantiate it on the illustrative dual-task of joint few-shot recognition and novel-view synthesis: given only one or few images of a novel object from arbitrary views with only category annotation, we aim to simultaneously learn an object classifier and generate images of the object from new viewpoints. </p>
                                        <p class="paraIndent normalcolor"> To this end, we propose bowtie networks that jointly learn 3D geometric and semantic representations with feedback in the loop. Experimental evaluation on challenging fine-grained recognition datasets demonstrates that our synthesized images are realistic from multiple viewpoints and significantly improve recognition performance as ways of data augmentation, especially in the low-data regime. We further show that our approach is flexible and can be easily extended to incorporate other tasks, such as style guided synthesis.</p>
                                        <p class="paraIndent normalcolor"> In this work, I am responsible for designing the whole architecture and fine tune the network. We have submitted a regular paper for ICLR 2021. </p>
                                        <p class="paraIndent normalcolor"> Find the paper <a target="_blank" href="https://arxiv.org/abs/2008.06981"><strong>here</strong></a>. </p>
                                    
                                    </div>
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/liftr.png"  class="img-responsive">
                                                <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div>

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <div class="meta">
                                                <h3>Marker-free Alignment for Electron Tomography</h3>
                                                <p>July 2018 - Dec. 2018</p>
                                                <p>Supervisor: Min Xu, Assistant Professor, CMU</p>
                                                <p>Propose a robust tracking and reconstruction method for the electron tomography.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="details">
                                        <img src="files/fig1.png" width="100%"> </p>
                                        <p class="paraIndent normalcolor"> Electron tomography is a widely used 3D macro-molecular structure reconstruction technology. The alignment of the tilt series is one of the most improvement processes that determine the tomography reconstruction quality. However, the data process for marker-free electron tomography remains a challenge, especially when the distortion of the projection has been proved and the correction becomes necessary. </p>
                                        <p class="paraIndent normalcolor"> During this projection, we propose a marker-free algorithm for the alignment of the tilt series. We first make an automatic detection and tracking of the stable ultrastructures with a deep neural network. We then refine the local patches of the projection parameters and produce the reprojection invariant landmarks. Finally, we rebuild the 3D cellular structure based on the landmarks and the projections. The proposed method obtains better performances than other state-of-the-art reconstruction algorithms.</p>
                                        <p class="paraIndent normalcolor">
                                        During this work, I propose and apply a deep neural network that can extract the deep features of each projection and find the interesting patches based on that.
                                        </p>
                                        <p class="paraIndent normalcolor">
                                            Our paper was accepted to ISMB 2019. Find the paper <a target="_blank" href="https://academic.oup.com/bioinformatics/article/35/14/i249/5529170"><strong>here</strong></a> </p>.
                                            </p>
                                    </div>
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/scholar.png"  class="img-responsive">
                                                <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div>

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <div class="meta">
                                                <h3>Facial Expression Recognition using 3D Re-centralization</h3>
                                                <p>Feb. 2018 - Sep. 2018</p>
                                                <p>Supervisor: Shaodi You, Senior Research Scientist, CSIRO</p>
                                                <p>Solve the problem of facial expression recognition in extreme lighting or orientations.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="details">
                                        <p><img src="files/fig3.png" width="100%"></p>
                                        <p class="paraIndent normalcolor"> Facial expression recognition is an important topic because it is the basis for many AI tasks. Numerous studies have been conducted in this area under the assumption that the human subject is always properly facing the camera. Such a condition, however, is too rigid because, in many real-life situations, the human face is not directly in front of the camera and maybe under extreme light conditions. However, few of the existing methods have considered the orientation of a face, thus limiting the performance of the previous models.</p>
                                        <p class="paraIndent normalcolor">To tackle this issue, we explore the fact that human expression is independent of the facial orientation. On the basis of this assumption, we propose a novel method to re-align facial images by reconstructing a 3D face model from a single image. Moreover, we implement an end to end deep neural network for single image facial expression recognition tasks, which utilize learning-based features, landmark features, and 3D features. A comprehensive evaluation toward three real-world datasets illustrates that the proposed model outperforms the state-of-the-art techniques in both large-scale and small-scale datasets. The superior of our model on effectiveness and robustness is also demonstrated in both laboratory conditions and wild images.</p>
                                        <p class="paraIndent normalcolor">This is an individual research project I took part in when I was exchanging in ANU. I am responsible for the whole project under the supervision of Shaodi. Our paper was accepted to ICCV 2019 workshops</p>
                                        <p class="paraIndent normalcolor"> Find the paper <a target="_blank" href="https://openaccess.thecvf.com/content_ICCVW_2019/html/CVPM/Bao_Single-Image_Facial_Expression_Recognition_Using_Deep_3D_Re-Centralization_ICCVW_2019_paper.html"><strong>here</strong>.</a>
                                    </div>
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/scissors.png"  class="img-responsive">
                                                <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div>

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <div class="meta">
                                                <h3>Multimodal Sentiment Analysis with Deep Temporal Convolution</h3>
                                                <p>Feb. 2018 - Sep. 2018</p>
                                                <p>Supervisor: Hua Xu, Associated Professor, Tsinghua University</p>
                                                <p>Video emotion analysis via visual, vocal and textual features.</p>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="details">
                                        <img src="files/fig2.png" width="100%"> </p>
                                        <p class="paraIndent normalcolor"> With the surge of social websites, hundreds of online videos are generated every day, which makes multimodal sentiment analysis an increasingly popular research hot spot. There are three main characteristics of outstanding multimodal sentiment analysis models: real-time analysis of sentiment, accurate extraction of multimodal features, and efficient feature fusion strategy. The current well-performing models still have room for improvement regarding these characteristics. </p>
                                        <p class="paraIndent normalcolor"> On the basis of this fact, in this project, we propose some improvement measures for each of these three aspects. We have designed a deep architecture to extract the features of the multimodal data, a temporal convolution neural network to improve the operational efficiency and also proposed an effective weighted fusion strategy. An entire model for multimodal sentiment analysis combining these three architectures is also proposed. Experimental results on the real-world dataset indicate that the proposed model outperforms the other state-of-the-art models in terms of accuracy, efficiency, and robustness. Targeted experiments also prove the superiority of the proposed deep feature-extracting network, temporal convolution network, and weighted fusion strategy.</p>
                                        <p class="paraIndent normalcolor"> In this work, I am responsible for designing the whole architecture and fine tune the network. We have orgnized our work and formed a paper. </p>
                                        <p class="paraIndent normalcolor"> Find the paper <a target="_blank" href="files/multimodal.pdf"><strong>here</strong></a>. </p>
                                    </div>
                                </li>

                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
</div>
</div>
</body>
</html>
