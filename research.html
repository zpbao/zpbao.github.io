<!DOCTYPE html>
<html lang="en">
<head>
    <title>Zhipeng Bao</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Zhipeng Bao">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="Zhipeng Bao's Homepage" />
    <meta name="keywords" content="Zhipeng Bao" />

    <link rel="shortcut icon" href="./files/cmu.png">

    <!--CSS styles-->
    <link rel="stylesheet" href="css/bootstrap.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/perfect-scrollbar-0.4.5.min.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <link rel="stylesheet" href="css/style.css">
    <link id="theme-style" rel="stylesheet" href="css/styles/default.css">


    <!--/CSS styles-->
    <!--Javascript files-->
    <script type="text/javascript" src="js/jquery-1.10.2.js"></script>
    <script type="text/javascript" src="js/TweenMax.min.js"></script>
    <script type="text/javascript" src="js/jquery.touchSwipe.min.js"></script>
    <script type="text/javascript" src="js/jquery.carouFredSel-6.2.1-packed.js"></script>

    <script type="text/javascript" src="js/modernizr.custom.63321.js"></script>
    <script type="text/javascript" src="js/jquery.dropdownit.js"></script>

    <script type="text/javascript" src="js/jquery.stellar.min.js"></script>
    <script type="text/javascript" src="js/ScrollToPlugin.min.js"></script>

    <script type="text/javascript" src="js/bootstrap.min.js"></script>

    <script type="text/javascript" src="js/jquery.mixitup.min.js"></script>

    <script type="text/javascript" src="js/masonry.min.js"></script>

    <script type="text/javascript" src="js/perfect-scrollbar-0.4.5.with-mousewheel.min.js"></script>

    <script type="text/javascript" src="js/magnific-popup.js"></script>
    <script type="text/javascript" src="js/custom.js"></script>

    <!--/Javascript files-->

</head>
<body>

<div id="wrapper">
    <a href="#sidebar" class="mobilemenu"><i class="icon-reorder"></i></a>

    <div id="sidebar">
        <div id="main-nav">
            <div id="nav-container">
                <div id="profile" class="clearfix">
                    <div class="portrate hidden-xs"></div>
                    <div class="title">
                        <h2>Zhipeng Bao</h2>
                        <h3>Carnegie Mellon University</h3>
                    </div>

                </div>
                <ul id="navigation">
                    <li>
                        <a href="index.html">
                            <div class="icon icon-user"></div>
                            <div class="text">About Me</div>
                        </a>
                    </li>

                    <li class="currentmenu">
                        <a href="research.html">
                            <div class="icon icon-book"></div>
                            <div class="text">Research</div>
                        </a>
                    </li>

                    <!-- <li>
                        <a href="publication.html">
                            <div class="icon icon-edit"></div>
                            <div class="text">Publications</div>
                        </a>
                    </li> -->

                    <li>
                        <a href="gallery.html">
                            <div class="icon icon-picture"></div>
                            <div class="text">Gallery</div>
                        </a>
                    </li>

                    <li>
                        <a href="contact.html">
                            <div class="icon icon-calendar"></div>
                            <div class="text">Contact Me</div>
                        </a>
                    </li>

                    <li>
                        <a href="cv-zhipengbao.pdf">
                            <div class="icon icon-download-alt"></div>
                            <div class="text">Download CV</div>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="social-icons">
            <ul>
                <li><a href="https://www.facebook.com/profile.php?id=100010655260380/"><i class="icon-facebook"></i></a></li>
                <li><a href="https://www.linkedin.com/in/zhipeng-bao-11030016a/"><i class="icon-linkedin"></i></a></li>
                <li><a href="https://github.com/zpbao"><i class="icon-github"></i></a></li>
            </ul>
        </div>
    </div>

<div id="main">

    <div id="research" class="page">
        <div class="pageheader">

            <div class="headercontent">

                <div class="section-container">
                    <h3 class="title">Research Projects</h2>
                </div>
            </div>
        </div>

        
        <div class="pagecontents">
            <div class="section color-2">
                <div class="section-container">
                    <div class="row">
                        <div class="col-md-12 col-md-offset-0">
                            <ul class="ul-withdetails">
                                
                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/lexicon.png" class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->
                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</h3>
                                                <p>NeurIPS 2024</p>
                                                <p class="paraIndent"><a href = "https://yunzeman.github.io/"> Yunze Man</a>, <a href = "https://zsh2000.github.io/"> Shuhong Zheng</a>, <b> Zhipeng Bao</b>, <a href = "http://www.cs.cmu.edu/~hebert"> Martial Hebert</a>, <a href = "https://lgui.web.illinois.edu/"> Liangyan Gui</a>, <a href = "https://yxw.web.illinois.edu/">Yu-Xiong Wang</a></p>
                                                <!-- <p>Incorporating multi-task learning to neural randiance fields for scene property synthesis.</p> -->
                                                <p><a target="_blank" href = "https://arxiv.org/abs/2409.03757">[pdf]</a><a target="_blank" href = "https://github.com/YunzeMan/Lexicon3D">[code]</a><a target="_blank" href = "https://yunzeman.github.io/lexicon3d/index.html">[project page]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/sepen.gif" class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->
                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>Separate-and-Enhance: Compositional Finetuning with Text-to-Image Diffusion Models</h3>
                                                <p>SIGGRAPH 2024</p>
                                                <p class="paraIndent"><b> Zhipeng Bao</b>, <a href = "https://yijunmaverick.github.io/">Yijun Li</a>, <a href = "https://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>, <a href = "https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>, <a href = "http://www.cs.cmu.edu/~hebert"> Martial Hebert</a></p>
                                                <!-- <p>Incorporating multi-task learning to neural randiance fields for scene property synthesis.</p> -->
                                                <p><a target="_blank" href = "https://camps.aptaracorp.com/ACM_PMS/PMS/ACM/SIGGRAPHCONFERENCEPAPERS24/133/d60517f0-feed-11ee-8182-16bb50361d1f/OUT/siggraphconferencepapers24-133.html">[pdf]</a><a target="_blank" href = "https://github.com/adobe/SeperateAndEnhance">[code]</a><a target="_blank" href = "https://zpbao.github.io/projects/SepEn/">[project page]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                </li>

                                


                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/mtvs.png" class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->
                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>Multi-task View Synthesis with Neural Radiance Fields</h3>
                                                <p>ICCV 2023</p>
                                                <p class="paraIndent"><a href = "https://zsh2000.github.io/"> Shuhong Zheng*</a>, <b> Zhipeng Bao*</b>, <a href = "http://www.cs.cmu.edu/~hebert"> Martial Hebert</a>, <a href = "https://yxw.web.illinois.edu/">Yu-Xiong Wang</a></p>
                                                <!-- <p>Incorporating multi-task learning to neural randiance fields for scene property synthesis.</p> -->
                                                <p><a target="_blank" href = "https://arxiv.org/abs/2303.15555">[pdf]</a><a target="_blank" href = "https://github.com/zsh2000/MuvieNeRF">[code]</a><a target="_blank" href = "https://zsh2000.github.io/mtvs.github.io/">[project page]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                    <!-- <div class="details">
                                        <img src="files/motok.png" width="100%"> </p>
                                        
                                        <p>Multi-task visual learning is a critical aspect of computer vision. Current research, however, predominantly concentrates on the multi-task dense prediction setting, which overlooks the intrinsic 3D world and its multi-view consistent structures, and lacks the capability for versatile imagination.
                                            In response to these limitations, we present a novel problem setting -- multi-task view synthesis (MTVS), which reinterprets multi-task prediction as a set of novel-view synthesis tasks for multiple scene properties, including RGB.
                                            To tackle the MTVS problem, we propose MuvieNeRF, a framework that incorporates both multi-task and cross-view knowledge to simultaneously synthesize multiple scene properties. MuvieNeRF integrates two key modules, the Cross-Task Attention (CTA) and Cross-View Attention (CVA) modules, enabling the efficient use of information across multiple views and tasks.
                                            Extensive evaluation on both synthetic and realistic benchmarks demonstrates that MuvieNeRF is capable of simultaneously synthesizing different scene properties with promising visual quality, even outperforming conventional discriminative models in various settings. Notably, we show that MuvieNeRF exhibits universal applicability across a range of NeRF backbones.</p>
                                        <p>Our paper was accepted to ICCV 2023.</p>
                                    </div> -->
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/motok.gif" class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->
                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>Objects Discovery from Motion-guided Tokens</h3>
                                                <p>CVPR 2023</p>
                                                <p class="paraIndent"> <b>Zhipeng Bao</b>, <a href = "https://pvtokmakov.github.io/home/"> Pavel Tokmakov</a>, <a href = "https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>, <a href = "https://adriengaidon.com/"> Adrien Gaidon</a>, <a href = "http://www.cs.cmu.edu/~hebert"> Martial Hebert</a></p>
                                                <!-- <p>Self-suppervised object-centric feature representations with a motion cues and VQ tokens.</p> -->
                                                <p><a target="_blank" href = "https://arxiv.org/abs/2303.15555">[pdf]</a><a target="_blank" href = "https://github.com/zpbao/MoTok">[code]</a><a target="_blank" href = "projects/MoTok/index.html">[project page]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                    <!-- <div class="details">
                                        <img src="files/motok.png" width="100%"> </p>
                                        
                                        <p>Object discovery -- separating objects from the background without manual labels -- is a fundamental open challenge in computer vision. Previous methods struggle to go beyond clustering of low-level cues, whether handcrafted (e.g., color, texture) or learned (e.g., from auto-encoders). In this work, we augment the auto-encoder representation learning framework with two key components: motion-guidance and mid-level feature tokenization. Although both have been separately investigated, we introduce a new transformer decoder showing that their benefits can compound thanks to motion-guided vector quantization. We show that our architecture effectively leverages the synergy between motion and tokenization, improving upon the state of the art on both synthetic and real datasets. Our approach enables the emergence of interpretable object-specific mid-level features, demonstrating the benefits of motion-guidance (no labeling) and quantization (interpretability, memory efficiency).</p>
                                        <p>Our paper was accepted to CVPR 2023. See the paper <a target="_blank" href="https://arxiv.org/abs/2303.15555"><strong>here</strong></a>.</p>
                                    </div> -->
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/sasnerf.png" class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>Beyond RGB: Scene-property Synthesis with Neural Radiance Fields</h3>
                                                <p>WACV 2023</p>
                                                <p class="paraIndent"><a href = "https://robo-alex.github.io/"> Mingtong Zhang*</a>, <a href = "https://zsh2000.github.io/"> Shuhong Zheng*</a>, <b> Zhipeng Bao</b>, <a href = "http://www.cs.cmu.edu/~hebert"> Martial Hebert</a>, <a href = "https://yxw.web.illinois.edu/">Yu-Xiong Wang</a></p>
                                                <!-- <p>Extend Neural Radiance Fields from color image systhesis to simultaneously synthesizing RGB and corresponding multi-task labels.</p> -->
                                                <p><a target="_blank" href = "https://openaccess.thecvf.com/content/WACV2023/html/Zhang_Beyond_RGB_Scene-Property_Synthesis_With_Neural_Radiance_Fields_WACV_2023_paper.html">[pdf]</a><a target="_blank" href = "https://github.com/zsh2000/SS-NeRF">[code]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                    <!-- <div class="details">
                                        <img src="files/nerf.png" width="100%"> </p>
                                        
                                        <p>Comprehensive 3D scene understanding, both geometrically and semantically, is important for various real-world applications such as robot perception. Most of the existing work has focused on developing data-driven discriminative models for different scene analysis problems. In this project, we provides a new way for scene analysis from a generative modeling perspective, by leveraging the recent progress on implicit 3D representation and neural rendering. </p>
                                        <p>Building upon the great success of Neural Radiance Fields (NeRFs), we develop scene analysis by synthesis with NeRF (SaS-NeRF) that is able to not only render photo-realistic RGB images from novel viewpoints, but also render various accurate scene properties (e.g., appearance, geometry and semantics) paired with the synthesized images. By doing so, we facilitate addressing a variety of scene understanding tasks under a unified framework, including semantic segmentation, surface normal estimation, reshading, 2D-keypoint detection, and edge detection. Our SaS-NeRF framework can be a powerful tool for bridging generative learning and discriminative learning and thus be beneficial to the investigation of a wide range of interesting problems, such as studying task relationships from a generative modeling perspective, facilitating downstream discriminative tasks as ways of data augmentation, and serving as auto-labeller.</p>
                                        <p>Our paper was accepted to WACV 2023. See the paper <a target="_blank" href="https://arxiv.org/abs/2206.04669"><strong>here</strong></a>.</p>
                                    </div> -->
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/mgm.png" class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>Generative Modeling for Multi-task Visual Learning</h3>
                                                <p>ICML 2022</p>
                                                <p class="paraIndent"><b> Zhipeng Bao</b>, <a href = "http://www.cs.cmu.edu/~hebert"> Martial Hebert</a>, <a href = "https://yxw.web.illinois.edu/">Yu-Xiong Wang</a></p>
                                                <!-- <p>Apply genertaive networks to facilitate multi-task visual learning.</p> -->
                                                <p><a target="_blank" href = "https://proceedings.mlr.press/v162/bao22c.html">[pdf]</a><a target="_blank" href = "https://github.com/zpbao/multi-task-oriented_generative_modeling">[code]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                    <!-- <div class="details">
                                        <img src="files/multitask.png" width="100%"> </p>
                                        <p>Generative modeling has recently shown great promise in computer vision, but it has mostly focused on synthesizing visually realistic images. In this project, motivated by multi-task learning of shareable feature representations, we consider a broader problem of learning a shared genera- tive model that is useful across various visual perception tasks. </p>
                                        <p>we propose a general multi-task oriented generative modeling (MGM) framework, by coupling a discriminative multi-task network with a generative network. While it is challenging to synthesize both RGB images and pixel-level annotations in multi-task scenarios, our framework enables us to use synthesized images paired with only weak annotations (i.e., image-level scene labels) to facilitate multiple visual tasks. Experimental evaluation on challenging multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework improves the performance of all the tasks by large margins, consistently outperforming state-of-the-art multi-task approaches.</p>
                                        <p>Our paper was accepted to ICML 2022. See the paper <a target="_blank" href="https://arxiv.org/abs/2106.13409"><strong>here</strong></a>.</p>
                                    </div> -->
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/dom.gif" class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->
                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>Discovering Objects that Can Move</h3>
                                                <p>CVPR 2022</p>
                                                <p class="paraIndent"> <b>Zhipeng Bao*</b>, <a href = "https://pvtokmakov.github.io/home/"> Pavel Tokmakov*</a>, <a href = "https://ajabri.github.io/"> Allan Jabri</a>,<a href = "https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>, <a href = "https://adriengaidon.com/"> Adrien Gaidon</a>, <a href = "http://www.cs.cmu.edu/~hebert"> Martial Hebert</a></p>
                                                <!-- <p>Self-suppervised object-centric feature representations with a weak learning signal from motion.</p> -->
                                                <p><a target="_blank" href = "https://openaccess.thecvf.com/content/CVPR2022/papers/Bao_Discovering_Objects_That_Can_Move_CVPR_2022_paper.pdf">[pdf]</a><a target="_blank" href = "https://github.com/zpbao/Discovery_Obj_Move">[code]</a><a target="_blank" href = "projects/CVPR22-Discovering/index.html">[project page]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                    <!-- <div class="details">
                                        <img src="files/tri_cvpr22.png" width="100%"> </p>
                                        
                                        <p>We studies the problem of object discovery -- separating objects from the background without manual labels. Existing approaches rely on appearance cues, such as color, texture and location, to group pixels into object-like regions. However, by relying on appearance alone, these methods fail to reliably separate objects from the background in cluttered scenes. This is a fundamental limitation, since the definition of an object is inherently ambiguous and context-dependent. To resolve this ambiguity, in this work we choose to focus on dynamic objects -- entities that are capable of moving independently in the world. </p>
                                        <p>We then scale the recent auto-encoder based frameworks for unsupervised object discovery from toy, synthetic images to complex, real world scenes by simplifying their architecture, and
                                            augmenting the resulting model with a weak learning signal from a motion segmentation algorithm. We demonstrate that, despite only capturing a small subset of the objects, this signal is enough to bias the model, which then learns to segment both moving and static instances of dynamic objects. 
                                            We show that this model scales to our newly collected, photo-realistic synthetic dataset with street driving scenarios. Additionally, we leverage ground truth segmentation and flow annotations in this dataset for thorough ablation and evaluation.
                                            Finally, our experiments on the real-world KITTI dataset demonstrate that the proposed approach outperforms both heuristic- and learning-based methods by capitalizing on motion cues.</p>
                                        <p>Our paper was accepted to CVPR 2022. See the paper <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Bao_Discovering_Objects_That_Can_Move_CVPR_2022_paper.pdf"><strong>here</strong></a>.</p>
                                    </div> -->
                                </li>
                                

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/fbnet.png"  class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>Generative Modeling for Joint Learning of Recognition and View-syntheis</h3>
                                                <p>ICLR 2021</p>
                                                <p class="paraIndent"><b> Zhipeng Bao</b>, <a href = "https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>, <a href = "http://www.cs.cmu.edu/~hebert"> Martial Hebert</a></p>
                                                <!-- <p>Provide an effective solution for the novel task of jointly recognition and view-synthesis, the model works well in low-data regime.</p> -->
                                                <p><a target="_blank" href = "https://arxiv.org/abs/2008.06981">[pdf]</a><a target="_blank" href = "https://github.com/zpbao/bowtie_networks">[code]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                    <!-- <div class="details">
                                        <img src="files/bowtie.png" width="100%"> </p>
                                        <p class="paraIndent normalcolor"> Generative modeling has recently shown great promise in computer vision, but its success is often limited to separate tasks. Motivated by multi-task learning of shareable feature representations, we consider a novel problem of learning a shared generative model across various tasks. We instantiate it on the illustrative dual-task of joint few-shot recognition and novel-view synthesis: given only one or few images of a novel object from arbitrary views with only category annotation, we aim to simultaneously learn an object classifier and generate images of the object from new viewpoints. </p>
                                        <p class="paraIndent normalcolor"> To this end, we propose bowtie networks that jointly learn 3D geometric and semantic representations with feedback in the loop. Experimental evaluation on challenging fine-grained recognition datasets demonstrates that our synthesized images are realistic from multiple viewpoints and significantly improve recognition performance as ways of data augmentation, especially in the low-data regime. We further show that our approach is flexible and can be easily extended to incorporate other tasks, such as style guided synthesis.</p>
                                        <p class="paraIndent normalcolor"> In this work, I am responsible for designing the whole architecture and fine tune the network. Our paper was accepted to ICLR 2021 as a poster paper. Find the paper <a target="_blank" href="https://arxiv.org/abs/2008.06981"><strong>here</strong></a>. </p>
                                    
                                    </div> -->
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/expression.png"  class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>Single-Image Facial Expression Recognition Using Deep 3D Re-Centralization</h3>
                                                <p>ICCV 2019 Workshops</p>
                                                <p><b>Zhipeng Bao</b>, <a href = "https://youshaodi.github.io/">Shaodi You</a>, <a href = "https://sites.google.com/view/linguedu/home">Gu Lin</a>, <a href = "https://bigdata.nankai.edu.cn/yangzl/list.htm">Zhenglu Yang</a></p>
                                                <!-- <p>Solve the problem of facial expression recognition in extreme lighting or orientations.</p> -->
                                                <p><a target="_blank" href = "https://openaccess.thecvf.com/content_ICCVW_2019/html/CVPM/Bao_Single-Image_Facial_Expression_Recognition_Using_Deep_3D_Re-Centralization_ICCVW_2019_paper.html">[pdf]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                    <!-- <div class="details">
                                        <p><img src="files/expression.png" width="100%"></p>
                                        <p class="paraIndent normalcolor"> Facial expression recognition is an important topic because it is the basis for many AI tasks. Numerous studies have been conducted in this area under the assumption that the human subject is always properly facing the camera. Such a condition, however, is too rigid because, in many real-life situations, the human face is not directly in front of the camera and maybe under extreme light conditions. However, few of the existing methods have considered the orientation of a face, thus limiting the performance of the previous models.</p>
                                        <p class="paraIndent normalcolor">To tackle this issue, we explore the fact that human expression is independent of the facial orientation. On the basis of this assumption, we propose a novel method to re-align facial images by reconstructing a 3D face model from a single image. Moreover, we implement an end to end deep neural network for single image facial expression recognition tasks, which utilize learning-based features, landmark features, and 3D features. A comprehensive evaluation toward three real-world datasets illustrates that the proposed model outperforms the state-of-the-art techniques in both large-scale and small-scale datasets. The superior of our model on effectiveness and robustness is also demonstrated in both laboratory conditions and wild images.</p>
                                        <p class="paraIndent normalcolor">This is an individual research project I took part in when I was exchanging in ANU. I am responsible for the whole project under the supervision of Shaodi. Our paper was accepted to ICCV 2019 workshops</p>
                                        <p class="paraIndent normalcolor"> Find the paper <a target="_blank" href="https://openaccess.thecvf.com/content_ICCVW_2019/html/CVPM/Bao_Single-Image_Facial_Expression_Recognition_Using_Deep_3D_Re-Centralization_ICCVW_2019_paper.html"><strong>here</strong>.</a>
                                    </div> -->
                                </li>

                                <li>
                                    <div class="row">
                                        <div class="col-sm-4 col-md-3">
                                            <div class="image">
                                                <img alt="image" src="img/projects/et.png"  class="img-responsive">
                                                <!-- <div class="imageoverlay">
                                                    <i class="icon icon-search"></i>
                                                </div> -->

                                            </div>
                                        </div>
                                        <div class="col-sm-8 col-md-9">
                                            <!-- <div class="meta"> -->
                                                <h3>A Joint Method for Marker-free Alignment of Tilt Series in Electron Tomography</h3>
                                                <p>ISMB 2019</p>
                                                <p><a href = "http://en.mis.sdu.edu.cn/info/1555/1393.htm">Renmin Han</a>, <b>Zhipeng Bao</b>, <a href = "https://scholar.google.com/citations?user=8gQLySoAAAAJ&hl=en">Xiangrui Zeng</a>, Tongxin Niu, <a href = "https://xulabs.github.io/">Min Xu</a>, <a href = "https://www.kaust.edu.sa/en/study/faculty/xin-gao">Xin Gao</a> </p>
                                                <!-- <p>Propose a robust tracking and reconstruction method for the electron tomography.</p> -->
                                                <p><a target="_blank" href = "https://academic.oup.com/bioinformatics/article/35/14/i249/5529170">[pdf]</a></p>
                                            <!-- </div> -->
                                        </div>
                                    </div>
                                    <!-- <div class="details">
                                        <img src="files/ET.png" width="100%"> </p>
                                        <p class="paraIndent normalcolor"> Electron tomography is a widely used 3D macro-molecular structure reconstruction technology. The alignment of the tilt series is one of the most improvement processes that determine the tomography reconstruction quality. However, the data process for marker-free electron tomography remains a challenge, especially when the distortion of the projection has been proved and the correction becomes necessary. </p>
                                        <p class="paraIndent normalcolor"> During this projection, we propose a marker-free algorithm for the alignment of the tilt series. We first make an automatic detection and tracking of the stable ultrastructures with a deep neural network. We then refine the local patches of the projection parameters and produce the reprojection invariant landmarks. Finally, we rebuild the 3D cellular structure based on the landmarks and the projections. The proposed method obtains better performances than other state-of-the-art reconstruction algorithms.</p>
                                        <p class="paraIndent normalcolor">
                                        During this work, I propose and apply a deep neural network that can extract the deep features of each projection and find the interesting patches based on that.
                                        </p>
                                        <p class="paraIndent normalcolor">
                                            Our paper was accepted to ISMB 2019. Find the paper <a target="_blank" href="https://academic.oup.com/bioinformatics/article/35/14/i249/5529170"><strong>here</strong></a>. </p>
                                            </p>
                                    </div> -->
                                </li>

                                

                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
</div>
</div>
</body>
</html>
